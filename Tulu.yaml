---
# Thank you for contributing!
# In filling out this yaml file, please follow the criteria as described here: 
# https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/tree/main/projects#criteria

# You're free to build on this work and reuse the data. It is licensed under CC-BY 4.0, with the
# stipulation that attribution should come in the form of a link to http://opening-up-chatgpt.github.io
# and a citation to the paper in which the initial dataset & criteria were published:

# Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. “Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.” In CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces. July 19-21, Eindhoven. doi: 10.1145/3571884.3604316 

system:
    name: Tulu
    link: https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B
    type: text 
    performanceclass: full
    basemodelname: Llama-3.1-405B
    endmodelname: Llama-3.1-Tulu-3-405B
    endmodellicense: AI2 ImpACT license
    releasedate: 2023-06
    notes: New model. TODO update fields. A new DPO model from AllenAI called Tülu

org:
    name: AllenAI
    link: https://allenai.org/
    notes:

# availability:
datasources_basemodel:
    class: closed
    link:
    notes: Based on Llama2 so nothing known about training data

datasources_endmodel:
    class: open
    link: https://github.com/allenai/open-instruct
    notes: Codebase used for fine-tuning this model and made available for others

weights_basemodel:
    class: partial
    link: https://github.com/meta-llama/
    notes: Base model made available via Meta (requires privacy-defying signup)

weights_endmodel:
    class: open
    link: https://huggingface.co/allenai/tulu-2-dpo-70b/tree/main
    notes: Fine-tuned weights available

trainingcode:
    class: partial
    link: https://github.com/allenai/open-instruct
    notes: Important effort to make available fine-tuning procedure and source code. Not seen a repository for base model training yet, so partial.

# documentation:
code:
    class: partial
    link: https://github.com/allenai/open-instruct
    notes: OpenInstruct code well-documented

hardware_architecture:
    class: partial
    link: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta
    notes: Post-llama2 aspects quite well described in papers, e.g. DPO based on Zephyr Beta, but base model is irrevocably closed

preprint:
    class: open
    link: https://arxiv.org/abs/2311.10702
    notes: Preprint covers the training of Tulu2

paper:
    class: closed
    link:
    notes: No peer-reviewed work found

modelcard:
    class: partial
    link: https://huggingface.co/allenai/tulu-2-70b
    notes: Model card offers little detail

datasheet:
    class: partial
    link: https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture
    notes: Some datasheets available (for instruction tuning), but base model entirely undocumented.

# access:
package:
    class: closed
    link: 
    notes: No separate package found 

api:
    class: open
    link:
    notes: Available via various APIs
    metaprompt: closed

licenses:
    class: partial
    link: https://allenai.org/impact-license
    notes: Meta Community License and AI2 Impact license

