---
# Thank you for contributing!
# In filling out this yaml file, please follow the criteria as described here:
# https://osai-index.eu/contribute

# You're free to build on this work and reuse the data. It is licensed under CC-BY 4.0, with the
# stipulation that attribution should come in the form of a link to https://osai-index.eu/
# and a citation to the peer-reviewed paper in which the dataset & criteria were published:

# Liesenfeld, A. and Dingemanse, M., 2024. Rethinking open source generative AI: open-washing and the EU AI Act. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (pp. 1774-1787).

system:
    name: GLM
    link: https://huggingface.co/THUDM/GLM-Z1-32B-0414
    type: text
    performanceclass: full
    basemodelname: GLM-4-32B-0414
    endmodelname: GLM-Z1-32B-0414
    endmodellicense: MIT License
    releasedate: 2025-04
    notes: English-Chinese reasoning model.
    
org:
    name: Zhipu AI
    link: https://github.com/THUDM
    notes: Zhipu AI, one of China's AI tigers.

# availability:
datasources_basemodel:
    class: closed
    link: 
        - http://doi.org/10.18653/v1/2022.acl-long.26
        - https://arxiv.org/abs/2406.12793
    notes: Training data not centrally made available, but described in 2022 ACL paper, appears to be mostly public datasets. Preprint also mentions "Our pre-training corpus consists of multilingual (mostly English and Chinese) documents from a mixture of different sources, including webpages, Wikipedia, books, code, and research papers", but does not go into more detail.

datasources_endmodel:
    class: closed
    link:
    notes: Docs mention "supervised fine-tuning, feedback bootstrap, and reinforcement learning wit human feedback", but none of the datasets used are clearly specified.

weights_basemodel:
    class: open
    link: https://huggingface.co/THUDM/GLM-4-32B-0414
    notes: Model weights made available through HuggingFace.

weights_endmodel:
    class: open
    link: https://huggingface.co/THUDM/GLM-Z1-32B-0414
    notes: Model weights made available through HuggingFace.

trainingcode:
    class: partial
    link: https://github.com/THUDM/GLM-4
    notes: Some code made available on Github.

# documentation:
code:
    class: closed
    link: https://github.com/THUDM/GLM-4/blob/main/README_en.md
    notes: No training code found.

hardware_architecture:
    class: closed
    link: 
    notes: No hardware architecture information found.

preprint:
    class: open
    link: https://arxiv.org/abs/2406.12793
    notes: Describes training data, architecture, and shows performance evaluation results of various ChatGLM models.

paper:
    class: partial
    link: https://aclanthology.org/2022.acl-long.26/
    notes: ACL 2022 paper describes the training of the GLM base model, but the RLHF portion is more recent (there is also a related ICLR paper for a newer generation https://openreview.net/forum?id=-Aw0rrrPUF)

modelcard:
    class: partial
    link: https://huggingface.co/THUDM/GLM-Z1-32B-0414
    notes: Model card is provided, however only contains general and inference information.

datasheet:
    class: closed
    link:
    notes: No datasheet found.

# access:
package:
    class: open
    link: https://ollama.com/library/glm4
    notes: Model available on Ollama.

api:
    class: partial
    link: https://bigmodel.cn/
    notes: Model has a paid API
    metaprompt: closed

licenses:
    class: open
    link: https://huggingface.co/THUDM/GLM-Z1-32B-0414
    notes: MIT License, an OSI-approved license.
