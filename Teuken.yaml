---
# Thank you for contributing!
# In filling out this yaml file, please follow the criteria as described here:
# https://osai-index.eu/contribute

# You're free to build on this work and reuse the data. It is licensed under CC-BY 4.0, with the
# stipulation that attribution should come in the form of a link to https://osai-index.eu/
# and a citation to the peer-reviewed paper in which the dataset & criteria were published:

# Liesenfeld, A. and Dingemanse, M., 2024. Rethinking open source generative AI: open-washing and the EU AI Act. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (pp. 1774-1787).

# Organization tags:
# - National origin: Germany
# - Contributor type: Non-academic (Research institution)

system:
    name: Teuken
    link: https://huggingface.co/openGPT-X/Teuken-7B-instruct-commercial-v0.4
    type: text
    performanceclass: full
    basemodelname: Teuken-7B-base
    endmodelname: Teuken-7B-instruct
    endmodellicense: Apache-2.0
    releasedate: 2024-09
    notes: Open-source multilingual LLM that claims to support all 24 official languages of the European Union.

org:
    name: OpenGPT-X
    link: https://opengpt-x.de/en/
    notes: Project aiming to develop LLMs in Germany.

# availability:
datasources_basemodel:
    class: partial
    link: https://arxiv.org/pdf/2410.08800
    notes: Dataset described as deriving from the CommonCrawl, but no filtered dataset provided. Either a filtered dataset or a fully reproducible and persistent data pipeline would be warranted.
    
datasources_endmodel:
    class: open
    link: https://huggingface.co/openGPT-X/Teuken-7B-instruct-commercial-v0.4#instruction-tuning-data
    notes: The Huggingface shows a table with all datasets used for the end model.
    
weights_basemodel:
    class: open
    link: https://huggingface.co/openGPT-X/Teuken-7B-base-v0.6
    notes: Available via Huggingface repository.

weights_endmodel:
    class: open
    link: https://huggingface.co/openGPT-X/Teuken-7B-instruct-commercial-v0.4
    notes: Available via Huggingface repository.

trainingcode:
    class: partial
    link: https://github.com/OpenGPTX/Megatron-LM/blob/main/examples/7B_EU24_juwels_part_3_fw_after3T.sbatch
    notes: SBATCH script with training code available at fork of Megatron-LM. However, no easily visible and easily navigable repository containing the code used to train the model is available.

# documentation:
code:
    class: closed
    link:
    notes: README of Megatron-LM repo containing training code is unchanged from base repo. More elaborate documentation would be warranted.

hardware_architecture:
    class: open
    link: https://arxiv.org/abs/2410.03730
    notes: Preprint shows architecture, providing details about design decisions and hyperparameters. 

preprint:
    class: open
    link: ["https://arxiv.org/abs/2410.03730", "https://arxiv.org/abs/2410.08928", "https://arxiv.org/abs/2410.08800"]
    notes: Three corresponding preprints, detailing the models, data, and evaluation.

paper:
    class: partial
    link: https://ecai2025.org/accepted-papers/
    notes: Peer-reviewed paper published in ECAI. Other publications only available as preprints.

modelcard:
    class: open
    link: https://huggingface.co/openGPT-X/Teuken-7B-instruct-commercial-v0.4
    notes: Detailed modelcard showing training details, data, technical specifications, and example usage.

datasheet:
    class: closed
    link:
    notes: No datasheet containing a detailed description of data collection and curation is found attached to a persistent version of the model data, as would be preferred here.

# access:
package:
    class: partial
    link: https://huggingface.co/openGPT-X/Teuken-7B-instruct-commercial-v0.4#how-to-get-started-with-the-model
    notes: Available through either Huggingface API or vLLM library, no proprietary release.

api:
    class: closed
    link:
    notes: No API found.
    metaprompts:

licenses:
    class: open
    link:
    notes: Apache 2.0, an OSI-approved license.
