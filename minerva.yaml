---
# Thank you for contributing!
# In filling out this yaml file, please follow the criteria as described here:
# https://osai-index.eu/contribute

# You're free to build on this work and reuse the data. It is licensed under CC-BY 4.0, with the
# stipulation that attribution should come in the form of a link to https://osai-index.eu/
# and a citation to the peer-reviewed paper in which the dataset & criteria were published:

# Liesenfeld, A. and Dingemanse, M., 2024. Rethinking open source generative AI: open-washing and the EU AI Act. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (pp. 1774-1787).

# Organization tags:
# - National origin: Italy
# - Contributor type: Academic (University)

system:
    name: Minerva
    link: https://huggingface.co/sapienzanlp/Minerva-7B-instruct-v1.0
    type: text 
    performanceclass: full
    basemodelname: Minerva-7B-base-v1.0
    endmodelname: Minerva-7B-instruct-v1.0
    endmodellicense: Apache-2.0
    releasedate: 2024-11
    notes: LLM pretrained from scratch on Italian.

org:
    name: Sapienza Natural Language Processing Group
    link: https://nlp.uniroma1.it/
    notes: Sapienza NLP, a university research group.

# availability:
datasources_basemodel:
    class: partial
    link: https://huggingface.co/sapienzanlp/Minerva-7B-base-v1.0#training-data
    notes: Data mixture is shared, but not all data in listed datasets is used and no data set containing selected data is provided.

datasources_endmodel:
    class: open
    link: https://huggingface.co/sapienzanlp/Minerva-7B-instruct-v1.0#sft-training
    notes: Data mixture is provided with links, though again it is not always indicated which parts of the datasets are used.

weights_basemodel:
    class: open
    link: https://huggingface.co/sapienzanlp/Minerva-7B-base-v1.0
    notes: Base LLM model made available for download
    
weights_endmodel:
    class: open
    link: https://huggingface.co/sapienzanlp/Minerva-7B-instruct-v1.0
    notes: Instruct version of the model made available

trainingcode:
    class: partial
    link: https://huggingface.co/sapienzanlp/Minerva-7B-instruct-v1.0
    notes: Model card contains very broad description of how the model was trained.

# documentation:
code:
    class: closed
    link: 
    notes: No explicit code is provided, so consequently no documentation of code exists.

hardware_architecture:
    class: partial
    link: https://huggingface.co/sapienzanlp/Minerva-7B-instruct-v1.0#model-architecture
    notes: Architecture described at a high level in model card.

preprint:
    class: open
    link: https://aclanthology.org/2024.clicit-1.77/
    notes: Paper published in ACL.

paper:
    class: open
    link: https://aclanthology.org/2024.clicit-1.77/
    notes: Paper published in ACL.

modelcard:
    class: open
    link: https://huggingface.co/sapienzanlp/Minerva-7B-instruct-v1.0
    notes: Model card is made available and provides insights into necessary components.

datasheet:
    class: closed
    link:
    notes: No datasheet available.

# access:
package:
    class: closed
    link: 
    notes: No package found.

api:
    class: closed
    link: 
    notes: No API available.
    metaprompt: closed

licenses:
    class: open
    link: https://huggingface.co/sapienzanlp/Minerva-7B-instruct-v1.0
    notes: Apache 2.0
