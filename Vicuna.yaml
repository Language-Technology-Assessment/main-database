---
# Thank you for contributing!
# In filling out this yaml file, please follow the criteria as described here: 
# https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/tree/main/projects#criteria

# You're free to build on this work and reuse the data. It is licensed under CC-BY 4.0, with the
# stipulation that attribution should come in the form of a link to http://opening-up-chatgpt.github.io
# and a citation to the paper in which the initial dataset & criteria were published:

# Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. “Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.” In CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces. July 19-21, Eindhoven. doi: 10.1145/3571884.3604316 

system:
    name: Vicuna
    link: https://huggingface.co/lmsys/vicuna-13b-v1.3
    type: text 
    performanceclass: full
    basemodelname: LLaMA
    endmodelname: ShareGPT
    endmodellicense: Non-commercial license
    releasedate: 2023-03
    notes: 'Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.'
 

org:
    name: LMSYS
    link: https://lmsys.org/
    notes: According to its website, 'The Large Model Systems Organisation develops large models and systems that are open, accessible and scalable'

# availability:
datasources_basemodel:
    class: partial
    link: https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md#training-dataset
    notes: Vicuna is fine-tuned LLaMA, and LLaMA in turn is based on 'publicly available datasets' that are not all specified or easily downloadable.

datasources_endmodel:
    class: closed
    link: https://github.com/lm-sys/FastChat#fine-tuning
    notes: From the documentation 'We will not release the ShareGPT dataset'. Also 'Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning. The training data is around 140K conversations collected from ShareGPT.com.'

weights_basemodel:
    class: open
    link: https://github.com/lm-sys/FastChat#vicuna-weights
    notes: Unlike Vicuna 13B v0, these weights do not require applying delta

weights_endmodel:
    class: closed
    link: https://github.com/lm-sys/FastChat#fine-tuning
    notes: No model weights are shared for the instruction tuning

trainingcode:
    class: open
    link: https://github.com/lm-sys/FastChat
    notes: Actively maintained repository

# documentation
code:
    class: open
    link: https://github.com/lm-sys/FastChat
    notes: Code is quite well-documented and released as part of the FastChat framework.

architecture:
    class: closed
    link:
    notes:

preprint:
    class: open
    link: https://arxiv.org/pdf/2306.05685.pdf
    notes: Preprint covers training of the Vicuna model.

paper:
    class: closed
    link:
    notes: No peer-reviewed paper.

modelcard:
    class: partial
    link: https://huggingface.co/lmsys/vicuna-13b-v1.3
    notes: Minimal model card, but many details are not provided or have to be pieced together from elsewhere.

datasheet:
    class: closed
    link:
    notes: No datasheet provided.

# access
package:
    class: open
    link: https://pypi.org/project/fschat/0.1.2/
    notes: Available via pip

api:
    class: partial
    link: https://github.com/lm-sys/FastChat#api
    notes: Support provided for several APIs OpenAI restful, HuggingFace, Langchain
    metaprompt: closed

licenses:
    class: partial
    link: https://github.com/lm-sys/FastChat#vicuna-weights
    notes: From the documentation 'Vicuna is based on LLaMA and should be used under LLaMA's model license.'

