---
# Thank you for contributing!
# In filling out this yaml file, please follow the criteria as described here:
# https://osai-index.eu/contribute

# You're free to build on this work and reuse the data. It is licensed under CC-BY 4.0, with the
# stipulation that attribution should come in the form of a link to https://osai-index.eu/
# and a citation to the peer-reviewed paper in which the dataset & criteria were published:

# Liesenfeld, A. and Dingemanse, M., 2024. Rethinking open source generative AI: open-washing and the EU AI Act. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (pp. 1774-1787).

# Organization tags:
# - National origin: International
# - Contributor type: Non-academic (Research institution)

system:
    name: Vicuna
    link: https://huggingface.co/lmsys/vicuna-13b-v1.3
    type: text 
    performanceclass: full
    basemodelname: Vicuna-13B
    endmodelname: Vicuna-13B-v1.3
    endmodellicense: Non-commercial license
    releasedate: 2023-03
    notes: 'Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.'
 

org:
    name: LMSYS
    link: https://lmsys.org/
    notes: According to its website, 'The Large Model Systems Organisation develops large models and systems that are open, accessible and scalable'

# availability:
datasources_basemodel:
    class: partial
    link: https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md#training-dataset
    notes: Vicuna is fine-tuned LLaMA, and LLaMA in turn is based on 'publicly available datasets' that are not all specified or easily downloadable.

datasources_endmodel:
    class: closed
    link: https://github.com/lm-sys/FastChat#fine-tuning
    notes: From the documentation 'We will not release the ShareGPT dataset'. Also 'Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning. The training data is around 140K conversations collected from ShareGPT.com.'

weights_basemodel:
    class: open
    link: https://github.com/lm-sys/FastChat#vicuna-weights
    notes: Unlike Vicuna 13B v0, these weights do not require applying delta

weights_endmodel:
    class: closed
    link: https://github.com/lm-sys/FastChat#fine-tuning
    notes: No model weights are shared for the instruction tuning

trainingcode:
    class: open
    link: https://github.com/lm-sys/FastChat
    notes: Actively maintained repository

# documentation
code:
    class: open
    link: https://github.com/lm-sys/FastChat
    notes: Code is quite well-documented and released as part of the FastChat framework.

hardware_architecture:
    class: closed
    link:
    notes:

preprint:
    class: open
    link: https://arxiv.org/pdf/2306.05685.pdf
    notes: Preprint covers training of the Vicuna model.

paper:
    class: closed
    link:
    notes: No peer-reviewed paper.

modelcard:
    class: partial
    link: https://huggingface.co/lmsys/vicuna-13b-v1.3
    notes: Minimal model card, but many details are not provided or have to be pieced together from elsewhere.

datasheet:
    class: closed
    link:
    notes: No datasheet provided.

# access
package:
    class: open
    link: https://ollama.com/library/vicuna
    notes: Model available on Ollama.

api:
    class: partial
    link: https://github.com/lm-sys/FastChat#api
    notes: Support provided for several APIs OpenAI restful, HuggingFace, Langchain
    metaprompt: closed

licenses:
    class: partial
    link: https://github.com/lm-sys/FastChat#vicuna-weights
    notes: From the documentation 'Vicuna is based on LLaMA and should be used under LLaMA's model license.'

